# Professional Summarization Prompt System
## Updated Instruction Set for Google LLM Integration

---

## Core System Prompt

Use this as the base system prompt for all summarization requests:

```python
SYSTEM_PROMPT = """You are a precise summarization engine designed for professional use.

Core Identity:
- You summarize content for curious users who value clarity and depth
- You are factual and never invent details
- You extract signal from noise, prioritizing substance over fluff

Hard Rules (Never Violate):
1. Never mention sponsors, ads, promotions, or calls-to-action
2. Never acknowledge that you skipped or ignored sponsor content
3. Never output sponsor language, brand names (e.g., Squarespace), or CTA phrases (e.g., "discount code")
4. Never use quotation marks of any kind (straight " or curly "")
5. Apostrophes in contractions are allowed (e.g., "don't", "it's")
6. Never output literal label text like "Title:" or "Headline:"
7. Never use emojis

Content Processing:
- Omit all sponsor messages, ad reads, promos, and promotional content entirely
- If transcript contains podcast ad reads, skip them without acknowledgment
- Focus on substantive claims, concepts, and insights
- Compress aggressively without hallucinating

Output Format:
- Use Markdown formatting
- Use ## for headings (never "Title:" labels)
- Use short paragraphs for readability
- Use bullet lists only when they improve scanability
- Avoid rigid templates or formulaic structures

Excerpts (Optional):
- If a standout line exists, include 1-2 short exact excerpts (max 25 words each)
- Format excerpts as italics using single asterisks: *like this*
- Never use quotation marks around excerpts
- Never include ad/sponsor/boilerplate excerpts
- If you cannot format an italic excerpt properly, omit it entirely

Final Validation:
- Remove any sponsor/ad references or mentions of skipping content
- Remove all quotation marks from output
- Ensure excerpts are italicized; if not possible, remove them
- Output only the summary, no meta-commentary
"""
```

---

## Dynamic Instruction Builder

Use this function to construct context-specific instructions for each request:

```python
def build_summarization_prompt(
    content: str,
    content_type: str = "text",  # "text", "webpage", "youtube", "pdf"
    length: str = "medium",      # "brief", "medium", "comprehensive"
    max_chars: int = None,
    output_language: str = "auto",  # "auto", "en", "es", etc.
    include_slide_markers: bool = False,
    custom_guidance: str = ""
) -> str:
    """
    Builds a complete prompt for the summarization request.
    
    Args:
        content: The text/transcript/article to summarize
        content_type: Type of content being summarized
        length: Desired summary length
        max_chars: Hard character limit (optional)
        output_language: Language for output ("auto" matches input)
        include_slide_markers: For presentations, preserve [slide:N] markers
        custom_guidance: Additional user-specific instructions
    
    Returns:
        Complete prompt string ready for LLM
    """
    
    instructions = []
    
    # Length directive
    length_guidance = {
        "brief": "Provide a brief summary (2-3 short paragraphs maximum).",
        "medium": "Provide a balanced summary with key insights and supporting details (3-5 paragraphs).",
        "comprehensive": "Provide a comprehensive summary covering all major points and nuances (no strict length limit, but stay concise)."
    }
    instructions.append(length_guidance.get(length, length_guidance["medium"]))
    
    # Character limit (if specified)
    if max_chars:
        instructions.append(f"Keep the summary under {max_chars} characters total.")
    
    # Content-type specific guidance
    content_type_guidance = {
        "youtube": "This is a video transcript. Focus on the main arguments and insights. Ignore any in-video sponsor segments or promotional content.",
        "webpage": "This is web page content. Extract the core information, ignoring navigation elements, ads, and boilerplate text.",
        "pdf": "This is a document. Preserve the logical structure and hierarchy of ideas.",
        "text": "This is user-provided text. Summarize the main points and key insights."
    }
    if content_type in content_type_guidance:
        instructions.append(content_type_guidance[content_type])
    
    # Slide markers (for presentations)
    if include_slide_markers:
        instructions.append("CRITICAL: If you encounter [slide:N] markers in the content, you MUST output those markers exactly on their own lines. Never output 'Slide X' or 'Slide X/Y' label lines.")
    
    # Output language
    if output_language == "auto":
        instructions.append("Output the summary in the same language as the input content.")
    else:
        lang_names = {
            "en": "English",
            "es": "Spanish",
            "fr": "French",
            "de": "German",
            "zh": "Chinese",
            "ja": "Japanese",
            "hi": "Hindi"
        }
        lang_name = lang_names.get(output_language, output_language)
        instructions.append(f"Output the summary in {lang_name}.")
    
    # Custom user guidance
    if custom_guidance:
        instructions.append(f"Additional user guidance: {custom_guidance}")
    
    # Final check reminder
    instructions.append("\nFinal check before outputting:")
    instructions.append("- Remove all sponsor/ad references")
    instructions.append("- Remove all quotation marks (straight and curly)")
    instructions.append("- Verify excerpts are italicized with single asterisks")
    instructions.append("- Use ## for headings (never 'Title:' labels)")
    instructions.append("- Output only the summary, no meta-commentary")
    
    # Assemble final prompt
    prompt = f"""{SYSTEM_PROMPT}

<instructions>
{chr(10).join(instructions)}
</instructions>

<content>
{content}
</content>

Output the summary now:"""
    
    return prompt
```

---

## Alternative: Structured JSON Output

If you prefer structured JSON output instead of Markdown (e.g., for programmatic parsing):

```python
STRUCTURED_SYSTEM_PROMPT = """You are a precise summarization engine that outputs structured JSON.

Core Rules:
- Output ONLY valid JSON, nothing else
- Never include meta-commentary, disclaimers, or source references
- Never mention sponsors, ads, or promotional content
- Never use quotation marks for emphasis within the JSON string values
- Apostrophes in contractions are allowed
- Prioritize claims and concepts over examples
- Compress aggressively without hallucinating

Output Format (EXACT structure required):
{
  "title": "[inferred title without quotes, max 10 words]",
  "takeaway": "[single sentence core insight]",
  "key_points": ["point 1", "point 2", "point 3"],
  "details": "[1-2 paragraphs expanding on key points]",
  "excerpts": ["*exact excerpt 1*", "*exact excerpt 2*"] or [],
  "limitations": "[notes on bias/limitations if any]" or ""
}

Field Requirements:
- title: Concise, descriptive, no quotation marks
- takeaway: Single sentence summarizing the core message
- key_points: Array of 3-6 strings, each a brief point
- details: 1-2 paragraphs providing context and depth
- excerpts: 0-2 standout quotes from original (max 25 words each), italicized with asterisks, or empty array
- limitations: Note any bias/caveats, or empty string if none

Never include:
- Sponsor/ad content or acknowledgment of omission
- Quotation marks within string values (except in JSON syntax)
- Emojis or special characters
- References to the source material's structure
"""

def build_structured_prompt(content: str, max_key_points: int = 5) -> str:
    """Build prompt for structured JSON output"""
    return f"""{STRUCTURED_SYSTEM_PROMPT}

Additional constraints:
- Maximum {max_key_points} key points
- If content has no excerpts worth highlighting, use empty array []
- If no limitations detected, use empty string ""

Content to summarize:
{content}

Output the JSON now (no markdown code blocks, just raw JSON):"""
```

---

## Follow-up Conversation Prompt

For handling follow-up questions in the side panel:

```python
FOLLOWUP_SYSTEM_PROMPT = """You are a helpful assistant answering follow-up questions about previously summarized content.

Core Rules:
- Answer questions based on the original summary and content
- Be concise and direct
- Never mention sponsors, ads, or promotional content
- Never use quotation marks for emphasis (apostrophes in contractions are OK)
- If asked about something not in the summary, say so clearly
- Stay focused on the substantive content

Response Format:
- Use Markdown formatting
- Short paragraphs preferred
- Bullet lists only when they clarify the answer
- No emojis
- If referencing the original content, use italics with single asterisks

Never do:
- Invent details not in the original content
- Include sponsor/ad information even if asked
- Use quotation marks of any kind (straight or curly)
- Provide overly long responses (aim for 2-4 paragraphs max)
"""

def build_followup_prompt(
    question: str,
    original_summary: str,
    conversation_history: list,
    original_content: str = None
) -> str:
    """
    Build prompt for follow-up question.
    
    Args:
        question: User's follow-up question
        original_summary: The summary that was previously generated
        conversation_history: List of {"role": "user/assistant", "content": "..."}
        original_content: Full original text (optional, for deep questions)
    """
    
    # Build conversation context
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-4:]  # Last 2 exchanges
        for msg in recent_history:
            role_label = "User" if msg["role"] == "user" else "Assistant"
            history_text += f"{role_label}: {msg['content']}\n\n"
    
    prompt = f"""{FOLLOWUP_SYSTEM_PROMPT}

Original Summary:
{original_summary}

{f"Full Original Content (for reference):{chr(10)}{original_content}{chr(10)}{chr(10)}" if original_content else ""}
Recent Conversation:
{history_text if history_text else "(No prior questions)"}

User's Question:
{question}

Provide a clear, concise answer based on the summary and original content:"""
    
    return prompt
```

---

## Server Implementation Example

Here's how to integrate these prompts into your Flask server:

```python
import google.generativeai as genai
import os

# Configure Gemini
genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))
model = genai.GenerativeModel('gemini-1.5-flash')

# Generation config for consistent behavior
generation_config = {
    'temperature': 0.3,  # Lower = more focused, less creative
    'top_p': 0.8,
    'top_k': 40,
    'max_output_tokens': 2048,
}

def call_google_llm(prompt: str) -> str:
    """Call Google LLM with the constructed prompt"""
    try:
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings={
                'HARASSMENT': 'BLOCK_NONE',
                'HATE_SPEECH': 'BLOCK_NONE',
                'SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                'DANGEROUS_CONTENT': 'BLOCK_NONE'
            }
        )
        return response.text.strip()
    except Exception as e:
        raise Exception(f"LLM generation failed: {str(e)}")


# ═══════════════════════════════════════════════════════════
# Flask Endpoints with New Prompt System
# ═══════════════════════════════════════════════════════════

@app.route('/summarize', methods=['POST'])
def summarize_text():
    """Summarize custom text input"""
    data = request.get_json()
    text = data.get('text', '')
    length = data.get('length', 'medium')  # brief, medium, comprehensive
    
    if not text or len(text.strip()) < 20:
        return jsonify({'error': 'Text must be at least 20 characters'}), 400
    
    try:
        prompt = build_summarization_prompt(
            content=text,
            content_type="text",
            length=length,
            max_chars=None
        )
        
        summary = call_google_llm(prompt)
        
        return jsonify({
            'summary': summary,
            'metadata': {
                'input_length': len(text),
                'word_count': len(text.split()),
                'model': 'gemini-1.5-flash',
                'length_setting': length
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/summarize-url', methods=['POST'])
def summarize_url():
    """Summarize web page"""
    data = request.get_json()
    url = data.get('url', '')
    length = data.get('length', 'medium')
    
    if not url:
        return jsonify({'error': 'No URL provided'}), 400
    
    try:
        # Fetch and extract text (your existing logic)
        text = fetch_url_text(url)
        
        prompt = build_summarization_prompt(
            content=text,
            content_type="webpage",
            length=length,
            max_chars=None
        )
        
        summary = call_google_llm(prompt)
        
        return jsonify({
            'summary': summary,
            'source_url': url,
            'metadata': {
                'input_length': len(text),
                'model': 'gemini-1.5-flash',
                'length_setting': length
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/summarize-youtube', methods=['POST'])
def summarize_youtube():
    """Summarize YouTube video transcript"""
    data = request.get_json()
    url = data.get('url', '')
    length = data.get('length', 'medium')
    
    if not url or ('youtube.com' not in url and 'youtu.be' not in url):
        return jsonify({'error': 'Invalid YouTube URL'}), 400
    
    try:
        # Extract transcript (your existing logic)
        transcript = fetch_youtube_transcript(url)
        
        prompt = build_summarization_prompt(
            content=transcript,
            content_type="youtube",
            length=length,
            max_chars=None
        )
        
        summary = call_google_llm(prompt)
        
        return jsonify({
            'summary': summary,
            'source_url': url,
            'metadata': {
                'input_length': len(transcript),
                'model': 'gemini-1.5-flash',
                'length_setting': length
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/follow-up', methods=['POST'])
def follow_up_question():
    """Handle follow-up questions with conversation history"""
    data = request.get_json()
    question = data.get('question', '')
    context = data.get('context', '')  # Original summary
    history = data.get('history', [])
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    if not context:
        return jsonify({'error': 'No context available'}), 400
    
    try:
        prompt = build_followup_prompt(
            question=question,
            original_summary=context,
            conversation_history=history,
            original_content=None  # Optionally pass full content if stored
        )
        
        answer = call_google_llm(prompt)
        
        return jsonify({
            'answer': answer,
            'metadata': {
                'model': 'gemini-1.5-flash',
                'question_length': len(question)
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

---

## Frontend Changes for Length Selection

Add length selector to your side panel UI:

```html
<!-- In sidepanel.html, add to quick-actions section -->
<div class="quick-actions">
  <button class="quick-btn" id="btnPage">Summarize This Page</button>
  <button class="quick-btn" id="btnYoutube">Summarize Video</button>
  
  <select id="lengthSelector" class="length-selector">
    <option value="brief">Brief</option>
    <option value="medium" selected>Medium</option>
    <option value="comprehensive">Comprehensive</option>
  </select>
</div>
```

CSS for selector:

```css
.length-selector {
  padding: 8px 12px;
  background: var(--bg-tertiary);
  border: 1px solid var(--border);
  border-radius: 6px;
  color: var(--text-secondary);
  font-size: 12px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.length-selector:hover {
  border-color: var(--accent);
}

.length-selector:focus {
  outline: none;
  border-color: var(--accent);
  box-shadow: 0 0 0 3px rgba(255, 107, 53, 0.1);
}
```

JavaScript to include length in API calls:

```javascript
// In sidepanel.js, modify callUrl and other functions:

async function callUrl(url) {
  showLoading('Fetching and summarizing page...');
  
  const length = document.getElementById('lengthSelector').value;
  
  try {
    const res = await fetch(`${API_BASE}/summarize-url`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ url, length })  // <-- Include length
    });
    // ... rest of logic
  } catch {
    showError('Cannot connect to server. Is it running?');
  }
}

// Apply same pattern to callYoutube and callText
```

---

## Testing the New Prompt System

Create a test script to validate prompt behavior:

```python
# test_prompts.py

from server import build_summarization_prompt, build_followup_prompt

# Test 1: Basic text summarization
test_content = """
This is a test article about machine learning. Machine learning is a subset of artificial intelligence 
that focuses on building systems that can learn from data. Modern ML systems use neural networks and 
deep learning to achieve state-of-the-art results. [SPONSOR: Check out Squarespace for 20% off]
The field has applications in healthcare, finance, and autonomous vehicles.
"""

prompt = build_summarization_prompt(
    content=test_content,
    content_type="text",
    length="brief"
)

print("=== TEST 1: Basic Summarization ===")
print(prompt)
print("\n\n")

# Test 2: Follow-up question
summary = "Machine learning is a subset of AI focused on learning from data using neural networks."
question = "What are the main applications mentioned?"

followup_prompt = build_followup_prompt(
    question=question,
    original_summary=summary,
    conversation_history=[],
    original_content=test_content
)

print("=== TEST 2: Follow-up Question ===")
print(followup_prompt)
```

Run with: `python test_prompts.py`

Expected behavior:
- Sponsor mention should trigger omission instructions
- Prompts should be clear and structured
- No quotation marks in output expectations

---

## Key Differences from Original Prompt

| Aspect | Original | Updated |
|--------|----------|---------|
| **Output Format** | Rigid JSON structure | Flexible Markdown (or optional JSON) |
| **Quotation Marks** | Forbidden entirely | Forbidden for emphasis, allowed in contrasts/apostrophes |
| **Excerpts** | Not mentioned | Encouraged (1-2 max, italicized) |
| **Sponsor Handling** | JSON field for it | Complete omission without acknowledgment |
| **Length Control** | Fixed compression | User-selectable (brief/medium/comprehensive) |
| **Heading Format** | JSON title field | Markdown ## headers (no "Title:" labels) |
| **Templates** | One rigid structure | Flexible, content-type aware |

---

## Production Checklist

Before deploying:

- [ ] Test with sponsor-heavy content (podcast transcripts)
- [ ] Verify quotation marks are removed from all outputs
- [ ] Test length selector with short and long content
- [ ] Confirm excerpts use single asterisks for italics
- [ ] Test follow-ups maintain context correctly
- [ ] Verify JSON output mode if using structured format
- [ ] Check error handling for API failures
- [ ] Test with non-English content if supporting multilingual
- [ ] Validate markdown formatting consistency
- [ ] Ensure slide markers (if used) are preserved

---

This prompt system is production-ready and handles the specific requirements you outlined while remaining flexible enough for different use cases.
